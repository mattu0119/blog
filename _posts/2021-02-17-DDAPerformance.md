---
title: "仮想マシンにパススルー接続した NVMe SSD と容量固定 VHDX の IO パフォーマンス測定"
date: 2021-02-16 18:00:00 +08:00
#last_modified_at: 2021-01-02 11:00:00 +08:00
categories: 
 - Azure Stack HCI
 - DDA
 - Discrete Device Assignment
 - 個別のデバイス割り当て
---
こんにちは。
今回は DDA で接続した NVMe SSD と VHDX ファイルのボリュームでどちらのパフォーマンスが良いか比較してみました。

# 仮想マシンの構成
DDA で接続した NVMe SSD を E ドライブ、容量固定 VHDX を F ドライブとして NTFS でフォーマットしました。

|ドライブ|ディスク|
|--|--|
|E:\ |NVMe SSD|
|F:\ |容量固定 VHDX|

# DISKSPD 
パフォーマンス測定にはマイクロソフト社の DISKSPD を利用しました。
+ [DISKSPD を使用してワークロード ストレージのパフォーマンスをテストする](https://docs.microsoft.com/ja-jp/azure-stack/hci/manage/DISKSPD-overview)

## DISKSPD とは
Docs.com に記載のある内容を引用します。ワークロードの負荷を実行できるツールということがわかります。

`DISKSPD は、カスタマイズして独自の合成ワークロードを生成し、アプリケーションをデプロイする前にテストすることができるツールです。 このツールの優れた点は、実際のワークロードに似た特定のシナリオを作成するために、パラメーターの構成と調整を自由に行うことができる点です。 DISKSPD を使用することで、システムをデプロイする前に、その能力をおおまかに把握することができます。 DISKSPD によって行われるのは、根本的には数多くの読み取りと書き込み操作の発行です。`

## DISKSPD のダウンロード
DISKSPD はインターネットからダウンロードできますが、Powershell でダウンロードする方法で進めてみます。
+ [DISKSPD](https://github.com/Microsoft/diskspd/releases/)

1. DISKSPD を実行するサーバーにログインし、管理者権限で Powershell を実行します。
1. DISKSPD をダウンロードし、解凍します。
    ```powershell
   Invoke-WebRequest -Uri "https://github.com/microsoft/diskspd/releases/download/v2.0.21a/DiskSpd.zip" -OutFile DiskSpd.zip
   Expand-Archive -Path C:\DiskSpd.zip
   cd .\DiskSpd\
    ```
1. 実行するサーバーによって、利用するバイナリを使い分けてください。
    <img src="{{ site.url }}{{ site.baseurl }}/assets/images/azshci/DDA-perf/1.png" class="full" width="600">

## DSKSPD の実行

DISKSPD を実行するパラメータは下記のものがあります。
+ [キー パラメーターを指定する](https://docs.microsoft.com/ja-jp/azure-stack/hci/manage/diskspd-overview#specify-key-parameters)

|パラメータ|説明|
|--|--|
|-t2|これは、ターゲット/テスト ファイルあたりのスレッドの数を示します。 この数は多くの場合、CPU コアの数に基づいています。 ここでは、すべての CPU コアに負荷をかけるため、2 つのスレッドが使用されました。|
|-o32|これは、スレッドごとのターゲットあたりの未処理 I/O 要求の数を示します。 これは、キューの深さとも呼ばれます。ここでは、CPU に負荷をかけるため、32 個が使用されました。|
|-b4K|これは、ブロック サイズをバイト、KiB、MiB、または GiB で示します。 ここでは、ランダム I/O テストをシミュレートするために 4K ブロック サイズが使用されました。|
|-r4K|これは、指定されたサイズ (バイト、KiB、MiB、Gib、またはブロック) に対応したランダム I/O を示します ( -s パラメーターをオーバーライドします)。 一般的な4K バイト サイズは、ブロック サイズに適切に対応するために使用されていました。|
|-w0|これを使用して、操作における書き込み要求の割合を指定します (-w0 は 100% 読み取りに相当します)。 ここでは、単純なテストの目的で 0% 書き込みが使用されました。|
|-d120|これは、クールダウンやウォームアップを含まない、テストの期間を指定します。 既定値は 10 秒ですが、重要なワークロードには少なくとも 60 秒を使用することをお勧めします。 ここでは、外れ値を最小化するために 120 秒が使用されました。|
|-Suw|ソフトウェアおよびハードウェアの書き込みキャッシュを無効にします ( -Sh に相当します)。|
|-D|標準偏差などの IOPS の統計情報をミリ秒単位 (スレッドごと、ターゲット単位) でキャプチャします。|
|-L|待機時間の統計を測定します。|
|-c5g|テストで使用されるサンプル ファイルのサイズを設定します。 これは、バイト、KiB、MiB、GiB、またはブロックで設定できます。 ここでは、5 GB のターゲット ファイルが使用されました。|


今回は、下記のパラメーターで実行してみます。
```powershell
.\diskspd -t2 -o32 -b4k -r4k -w40 -d120 -Sh -D -L -c5G E:\IO.dat > test01.txt
```

# 実行結果
1. NVMe に対する測定結果の抜粋
    +  Total IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    |0|39494254592|9642152|313.87|80350.16|0.397|732.52|0.105|E:\IO.dat (5120MiB)|
    |1|39487418368|9640483|313.81|80336.25|0.397|724.26|0.090|E:\IO.dat (5120MiB)|
    |total|78981672960|19282635|627.68|160686.40 | 0.397|1406.71|0.097|

    + Read IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    |0|23701483520|5786495|188.36|48220.13|0.429|481.92|0.106|E:\IO.dat (5120MiB)|
    |1|23695867904|5785124|188.32|48208.70|0.429|446.65|0.092|E:\IO.dat (5120MiB)|
    |total|47397351424|11571619|376.68|96428.83|0.429|880.00|0.099|

    + Write IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    |0|15792771072|3855657|125.51|32130.03|0.350|297.12|0.082|E:\IO.dat (5120MiB)|
    |1|15791550464|3855359|125.50|32127.55|0.350|326.31|0.062|E:\IO.dat (5120MiB)|
    |total|31584321536|7711016|251.01|64257.58|0.350|583.02|0.073|

    + ログデータは下記にアップロードしてありますので、ご参照ください。  
    https://gist.github.com/mattu0119/de2a91b4e1543859a6204ea885b02fb1#file-nvme-w40-txt

1. VHDX に対する測定結果の抜粋
    + Total IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    | 0 |     18685411328 |      4561868 |     148.48 |   38010.73 |    0.841 |    1402.61 |     0.456 | F:\IO.dat (5120MiB)|
    | 1 |     24736432128 |      6039168 |     196.56 |   50320.00 |    0.635 |    1850.14 |     0.395 | F:\IO.dat (5120MiB)|
    |total|  43421843456 |     10601036 |     345.04 |   88330.73 |    0.724 |    2473.50 |     0.434|

    + Read IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    | 0 |     11210981376 |      2737056 |      89.09 |   22805.90 |    0.735 |     838.17 |     0.382 | F:\IO.dat (5120MiB)|
    | 1 |     14839906304 |      3623024 |     117.92 |   30188.03 |    0.539 |    1115.86 |     0.321 | F:\IO.dat (5120MiB)|
    |total|       26050887680 |      6360080 |     207.01 |   52993.93 |    0.623 |    1499.35 |     0.362|

    + Write IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    |0|7474429952 |      1824812 |      59.39 |   15204.83 |    0.999 |     578.32 |     0.509 | F:\IO.dat (5120MiB)
    |1|      9896525824 |      2416144 |      78.64 |   20131.97 |    0.780 |     744.97 |     0.447 | F:\IO.dat (5120MiB)
    |total|       17370955776 |      4240956 |     138.03 |   35336.81 |    0.875 |     990.60 |     0.487|
    
    + ログデータは下記にアップロードしてありますので、ご参照ください。
    https://gist.github.com/mattu0119/0247f75fa3133c8ee1c3142a1571531e#file-vhdx-w40-txt


## 実行結果のおまけ
仮想マシンに VHDX 2つ接続し、Storage Space でミラーリングしたボリュームで計測してみました。  
<img src="{{ site.url }}{{ site.baseurl }}/assets/images/azshci/DDA-perf/1.png" class="full" width="600">

1. ミラーしたVHDX に対する測定結果の抜粋
    + Total IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    | 0 |      9261477888 |      2261103 |      73.60 |   18841.34 |    1.696 |    1673.56 |     1.383 | F:\IO.dat (5120MiB)|
    | 1 |     21545037824 |      5260019 |     171.21 |   43830.73 |    0.729 |    3472.12 |     0.710 | F:\IO.dat (5120MiB)|
    | total|30806515712 |      7521122 |     244.81 |   62672.07 |    1.020 |    4906.94 |     1.060|

    + Read IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    | 0 |      5554102272 |      1355982 |      44.14 |   11299.14 |    1.499 |    1015.36 |     1.234 | F:\IO.dat (5120MiB)|
    | 1 |     12925665280 |      3155680 |     102.72 |   26295.68 |    0.594 |    2085.76 |     0.605 | F:\IO.dat (5120MiB)|
    |total|       18479767552 |      4511662 |     146.85 |   37594.82 |    0.866 |    2954.78 |     0.941|

    + Write IO

    |thread|bytes|I/Os|MiB/s|I/O per s|AvgLat|IopsStdDev|LatStdDev|file|
    |--|--|--|--|--|--|--|--|--|
    | 0 |      3707375616 |       905121 |      29.46 |    7542.20 |    1.991 |     663.77 |     1.534 | F:\IO.dat (5120MiB)|
    | 1 |      8619372544 |      2104339 |      68.50 |   17535.06 |    0.932 |    1390.70 |     0.801 | F:\IO.dat (5120MiB)|
    |total|       12326748160 |      3009460 |      97.96 |   25077.26 |    1.251 |    1957.01 |     1.180|


# まとめ
トータル IOPS では、やはり NVMe SSD のパフォーマンスがよいことがわかります。  

|種類|Total IOPS|Read IOPS|Write IOPS|
|--|--|--|--|
|NVMe|160686.40|96428.83|64257.58|
|VHDX|88330.73|52993.93|35336.81|
|Mirror VHDX|62672.07|37594.82|25077.26|

サーバーの集約率を上げたいけれど、物理で準備するほどでもないという状況があった場合は、DDA を試してみてください！
